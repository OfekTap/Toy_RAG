{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f5aee2-1a23-4164-82c7-d226675ba5bd",
   "metadata": {},
   "source": [
    "# RAG implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a51c9-ca1f-4b6b-868a-396baabd15c6",
   "metadata": {},
   "source": [
    "In this file, we test simple RAG implementation using Pinecone and Cohere LLM. \\\n",
    "We use simple-Wikipedia dataset and for each article, we take only the first 4 paragraphs.\\\n",
    "Because simple Wikipedia has relatively small paragraphs we wanted to see how using only the first paragraphs will affect the RAG, instead of chunking it to different paragraphs and embedding them. \\\n",
    "In this toy example, we took only the first 400 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc70f17f-0deb-44ac-b89e-9404722adcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5753df1b-565d-46a5-81d6-7d1d6f3618ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "399da9b2-6bf9-41ab-b6a9-1d522d66544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cohere_api_key.txt\") as f:\n",
    "    COHERE_API_KEY = f.read().strip()\n",
    "with open(\"pinecone_api_key.txt\") as f:\n",
    "    PINECONE_API_KEY = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8cb129a-7c17-4db9-8ecd-f7a4b8e092d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load encoder model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73917884-67ed-449f-91e0-d7629219290a",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "### We use wikipedia dataset. We take only the first k paragraph from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a654cdf-5575-483b-8eda-0dddf521f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_embedd_dataset(\n",
    "        dataset_name: str = 'cnn_dailymail',\n",
    "        split: str = 'train',\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "        text_field: str = 'highlights',\n",
    "        rec_num: int = 400,\n",
    "        subset = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load a dataset and embedd the text field using a sentence-transformer model\n",
    "    Args:\n",
    "        dataset_name: The name of the dataset to load\n",
    "        split: The split of the dataset to load\n",
    "        model: The model to use for embedding\n",
    "        text_field: The field in the dataset that contains the text\n",
    "        rec_num: The number of records to load and embed\n",
    "        subset: the subset of the dataset to load. default is None\n",
    "    Returns:\n",
    "        tuple: A tuple containing the dataset and the embeddings\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"Loading and embedding the dataset\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    if subset is not None:\n",
    "        dataset = load_dataset(dataset_name, subset,split=split)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name,split=split)\n",
    "\n",
    "    # Function to take only the first k paragraphs\n",
    "    def take_first_k_paragraphs(text,k=4):\n",
    "        paragraphs = text.split('\\n') \n",
    "        return '\\n'.join(paragraphs[:k])\n",
    "    \n",
    "    # Apply the function to the text field\n",
    "    dataset = dataset.map(lambda x: {text_field: take_first_k_paragraphs(x[text_field],k=4)})\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Embed the first `rec_num` rows of the dataset  \n",
    "    embeddings = model.encode(dataset[text_field][:rec_num])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return dataset, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "010bcaa9-825d-4337-b066-ca8a3402f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding the dataset\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "DATASET_NAME = \"graelo/wikipedia\"\n",
    "subset = \"20230601.simple\"\n",
    "\n",
    "dataset, embeddings = load_and_embedd_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    rec_num=400,\n",
    "    split='train',\n",
    "    model=model,\n",
    "    text_field = 'text',\n",
    "    subset = subset\n",
    ")\n",
    "shape = embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68fd0c6c-eed4-4749-99c2-297358d0816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings shape: (400, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "500c8de0-ebec-46d4-83b9-3e49c8e2a3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    April is the fourth month of the year in the J...\n",
       "1    Art is a creative activity and technical skill...\n",
       "2    Air is the Earth's atmosphere. Air is a mixtur...\n",
       "3    Alan Mathison Turing OBE FRS (London, 23 June ...\n",
       "4    Adobe Illustrator is a computer program for ma...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = dataset.to_pandas()\n",
    "pd_dataset['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e574494e-991d-4bc4-a47d-2f9b0cc2b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air is the Earth's atmosphere. Air is a mixture of many gases and tiny dust particles. It is the clear gas in which living things live and breathe. It has an indefinite shape and volume. It has mass and weight, because it is matter. The weight of air creates atmospheric pressure. There is no air in outer space.\n",
      "\n",
      "Atmosphere is a mixture of about 78% nitrogen, 21% of oxygen, and 1% other gases, such as Carbon Dioxide.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd_dataset['text'].head(7)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a45c7-5378-47b0-8c6f-1180f2f91c08",
   "metadata": {},
   "source": [
    "## Creating index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "67dddd72-10af-4e9a-a3f0-4d52c789bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index(\n",
    "        index_name: str,\n",
    "        dimension: int,\n",
    "        metric: str = 'cosine',\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pinecone index if it does not exist\n",
    "    Args:\n",
    "        index_name: The name of the index\n",
    "        dimension: The dimension of the index\n",
    "        metric: The metric to use for the index\n",
    "    Returns:\n",
    "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
    "    \"\"\"\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    print(\"Creating a Pinecone index...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            # Remember! It is crucial that the metric you will use in your VectorDB will also be a metric your embedding\n",
    "            # model works well with!\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "    print(\"Done!\")\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6cc53e82-effd-45ed-8bd9-3aa061c37a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Pinecone index...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'wiki-dataset'\n",
    "\n",
    "# Create the vector database\n",
    "# We are passing the index_name and the size of our embeddings\n",
    "pc = create_pinecone_index(INDEX_NAME, shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e4098431-34ec-4158-8ed7-3072d09e3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_vectors(\n",
    "        index: Pinecone,\n",
    "        embeddings: np.ndarray,\n",
    "        dataset: dict,\n",
    "        text_field: str = 'highlights',\n",
    "        batch_size: int = 128\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a pinecone index\n",
    "    Args:\n",
    "        index: The pinecone index object\n",
    "        embeddings: The embeddings to upsert\n",
    "        dataset: The dataset containing the metadata\n",
    "        batch_size: The batch size to use for upserting\n",
    "    Returns:\n",
    "        An updated pinecone index\n",
    "    \"\"\"\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "    shape = embeddings.shape\n",
    "    \n",
    "    ids = [str(i) for i in range(shape[0])]\n",
    "    meta = [{text_field: text} for text in dataset[text_field]]\n",
    "    \n",
    "    # create list of (id, vector, metadata) tuples to be upserted\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    for i in tqdm(range(0, shape[0], batch_size)):\n",
    "        i_end = min(i + batch_size, shape[0])\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4b9a82a-b272-4ac4-b941-8ad1a309cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:02<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'wiki-dataset'\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index_upserted = upsert_vectors(index, embeddings, dataset,text_field='text',batch_size=2**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7948ebf-0e60-4ba1-aa5e-0469dc4099e1",
   "metadata": {},
   "source": [
    "## Making questions\n",
    "### We want to evaluate the model with questions it can answer in a systematic way.\n",
    "This part asks the LLM to make questions it can answer using the RAG data. We kept only those who their answer was different from the normal LLM answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23f8d0d2-85b5-4358-b3c9-1d0eea3afc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LLM\n",
    "import cohere\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af30f05b-7028-4314-a38f-ef3ae28edb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'José María de la Torre Martín (9 September 1952 – 14 December 2020) was a Mexican Roman Catholic bishop. De la Torres Martín was born in Mexico City. He became a priest in 1980. He was titular bishop of Panatoria and as auxiliary bishop of the Roman Catholic Archdiocese of Guadalajara, Mexico from 2002 to 2008 and as bishop of the Roman Catholic Diocese of Aguascalientes, Mexico, from 2008 until his death in 2020.\\n\\nDe la Torre Martín died on 14 December 2020 from COVID-19 in Aguascalientes, Mexico at the age of 68.\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(pd_dataset.sample(1)['text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e13874bb-4998-4af9-a787-9718a894eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question():\n",
    "    query = \"Generate a simple question from this paragraph, that is not specific to the paragraph but the answer to it is in the paragraph and provide answer \\n\"\n",
    "    sel_text = pd_dataset.sample(1)['text'].values[0]\n",
    "    query+= sel_text\n",
    "    \n",
    "    response = co.chat(\n",
    "            model='command-r-plus',\n",
    "            message=query,\n",
    "        )\n",
    "    print(\"response: \",response.text)\n",
    "    print(\"=====\")\n",
    "    print(\"Source:\")\n",
    "    print(sel_text)\n",
    "def ask_question(query):\n",
    "    response = co.chat(\n",
    "        model='command-r-plus',\n",
    "        message=query,\n",
    "    )\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75978116-3449-4321-89b9-d5c0c9243630",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f1850ea-a01a-4edc-8021-5d3bf04be785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  Question: What is the name of the district that Rocourt used to be a part of?\n",
      "Answer: The district of Porrentruy.\n",
      "=====\n",
      "Source:\n",
      "Rocourt was a municipality of the district of Porrentruy in the canton of Jura in Switzerland. On 1 January 2018, the former municipality of Rocourt merged into the municipality of Haute-Ajoie.\n",
      "\n",
      "References\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85f597ea-2417-4a4e-be3d-a2c58c3a9531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liège\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is the name of the district that Rocourt used to be a part of?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b7030-fce4-43f9-b2cc-3671ca276e01",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37de51b4-5df2-47d9-b1c7-bf31375328ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  Question: When was Leas Cliff Hall opened? \n",
      "Answer: Leas Cliff Hall was opened on July 13, 1927, by Prince Henry, Duke of Gloucester.\n",
      "=====\n",
      "Source:\n",
      "Leas Cliff Hall is an entertainment and function venue in Folkestone, on the Kent coast of England.\n",
      "\n",
      "History\n",
      "The Leas Shelter was built in 1894. In 1924, it was decided that a larger hall was needed. 28 months later, the building was finished. It was opened on 13 July 1927 by Prince Henry, Duke of Gloucester.\n"
     ]
    }
   ],
   "source": [
    "generate_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e822216-2fbd-466e-bc07-e3dead603fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leas Cliff Hall was opened on Thursday, July 6, 1927.\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"When was Leas Cliff Hall opened? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52d798-7a3c-470e-9d94-3819684377cf",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "694712ba-cc3b-45f2-980e-a3e1896eebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  Question: Who was known as the \"Walking Bible\"?\n",
      "Answer: Jack Van Impe.\n",
      "=====\n",
      "Source:\n",
      "Jack Leo Van Impe ( ; February 9, 1931 – January 18, 2020) was an American televangelist. He was known for his half-hour weekly television series Jack Van Impe Presents which was a commentary on the news of the week through with a twist of the Bible. He was known as the \"Walking Bible\", having memorized most of the King James Version of the Bible.\n",
      "\n",
      "Van Impe died on January 18, 2020 in Royal Oak, Michigan at a hospital from problems caused by a fall at the age of 88.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1e012eb4-50b3-4342-87f0-3d7e3d567e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Müller was known as the \"Walking Bible\" due to his remarkable memorization and recall of large portions of the Bible. Müller, a Christian evangelist and director of an orphanage in Bristol, England, in the 19th century, had a deep devotion to Scripture and is known for his faith and dedication to serving the needy. He attributed his ability to recall Bible verses to his habit of regularly reading and meditating on the Bible.\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Who was known as the Walking Bible?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e52ff-99e3-40ee-b3b9-964fe0d3de3a",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2bd3f2d5-dcb2-4e9b-9944-864066a3120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:  Question: What is a figure of speech that compares two different things?\n",
      "Answer: A simile.\n",
      "=====\n",
      "Source:\n",
      "A simile is a figure of speech that compares two different things, usually by using the words 'like' or 'as'. It is used to make a direct and clear comparison between two things .Similes may be confused with metaphors, which do the same kind of thing. Similes use comparisons, with the words 'like' or 'as'. Metaphors use indirect comparisons, without the words 'like' or 'as'.\n",
      "\n",
      "Similes:\n",
      "Like a hungry wolf, he ate the food.\n"
     ]
    }
   ],
   "source": [
    "generate_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "844d1acc-75a0-4bd0-b8ec-5f38dd00c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A figure of speech that compares two different things is a metaphor.\n"
     ]
    }
   ],
   "source": [
    "ask_question(\" What is a figure of speech that compares two different things?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846093d-4f5a-417f-b7ba-663d751c45c6",
   "metadata": {},
   "source": [
    "# Implemeting RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7176dd1-6a05-48c3-86f3-995639605e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(\n",
    "        query: str,\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "        index=None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Augment the prompt with the top 3 results from the knowledge base\n",
    "    Args:\n",
    "        query: The query to augment\n",
    "        index: The vectorstore object\n",
    "    Returns:\n",
    "        str: The augmented prompt\n",
    "    \"\"\"\n",
    "    results = [float(val) for val in list(model.encode(query))]\n",
    "    \n",
    "    # get top 3 results from knowledge base\n",
    "    query_results = index.query(\n",
    "        vector=results,\n",
    "        top_k=3,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )['matches']\n",
    "    text_matches = [match['metadata']['text'] for match in query_results]\n",
    "    \n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\\n\".join(text_matches)\n",
    "    \n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "    If the answer is not included in the source knowledge - say that you don't know.\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt, source_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bff52-821a-46b8-a2f7-d05b0f3f78cb",
   "metadata": {},
   "source": [
    "## Note\n",
    "We will see one example that did work and one example that did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a515c2-cf3a-4040-b4c4-9197a0f83dc6",
   "metadata": {},
   "source": [
    "On the first time the RAG was not useful because it embeds the whole text as the data to retrieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9197e35c-89be-4f39-9ef1-4110de39b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  I don't know.\n",
      "Source: \n",
      "Coden is a small fishing village near Bayou la Batre, Alabama, USA. It is about 20 miles southwest of Mobile, near the Alabama/Mississippi border. The name of the town comes from Coq d'Inde, which is French for \"Turkey\".\n",
      "\n",
      "Around 1900, the area was known as a resort, which is a place people go to on their vacations.  The Rolston Hotel brought visitors from all over the region. When it was destroyed by a hurricane, the community fell on hard times. The Rolston Hotel property now belongs to the City Of Bayou La Batre and is a park that  is  attracting people from other areas who want cool ocean breezes and peace that originally brought visitors. It is nice because it has the gentle sound of the water of Portersville Bay, fishing, and relaxation. Fresh seafood can be found on Shell Belt Road from fishing boats returning to Bayou Coden. Coden is on the southern shore of the mainland, across the Mississippi Sound from Dauphin Island and is one stop along Alabama's Coastal Birding Trail.\n",
      "\n",
      "\n",
      "Vienna (;  ; Central Austro-Bavarian: Wean ; Viennese German and Austrian German: Wian []) is the capital and largest city of Austria.\n",
      "\n",
      "It is in the east of the country on the river Danube. More than 1,800,000 people live there (2016). It is an administrative district (Bundesland) of its own.\n",
      "\n",
      "\n",
      "Montreal (, spelled Montréal in French) is a city in the country of Canada. It is the largest city in the province of Quebec and the second-largest city in Canada. It is the second-largest French-speaking city in the world after Paris.\n",
      "\n",
      "Montreal is built on an island sitting in the Saint Lawrence River. More than three million people live in the Montreal region. At the centre of Montreal is a mountain called Mount Royal. The suburb of Westmount, is a very affluent suburb of Quebec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the name of the district that Rocourt used to be a part of?\"\n",
    "augmented_prompt, source_knowledge = augment_prompt(query, model=model, index=index)\n",
    "response = co.chat(\n",
    "        model='command-r-plus',\n",
    "        message=augmented_prompt,\n",
    "    \n",
    "    )\n",
    "print(\"Response: \" ,response.text)\n",
    "print(\"Source: \\n\" +source_knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf0c31-3396-486c-8337-b4bb24000a29",
   "metadata": {},
   "source": [
    "### Implementing more complex RAG\n",
    "#### This code will first find the noun we need information on and then embed it to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1e59a90-920d-4a87-b81b-d1e945d7fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(query,model=model, index=index):\n",
    "    prompt = \"Given the following question, return only the name of the noun we need information on in order to solve the question \\n\" + query\n",
    "    response = co.chat(\n",
    "        model='command-r-plus',\n",
    "        message=prompt,\n",
    "    )\n",
    "    print(\"needed noun: \",response.text)\n",
    "    augmented_prompt,source_knowledge = augment_prompt(query, model=model, index=index)\n",
    "    response = co.chat(\n",
    "        model='command-r-plus',\n",
    "        message=augmented_prompt,\n",
    "    \n",
    "    )\n",
    "    print(\"Response: \" ,response.text)\n",
    "    print(\"Source: \\n\" +source_knowledge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "99cd81ee-b34c-4cbd-9e55-1e86a370c61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed noun:  comparison\n",
      "Response:  A figure of speech that compares two different things is a simile.\n",
      "Source: \n",
      "A simile is a figure of speech that compares two different things, usually by using the words 'like' or 'as'. It is used to make a direct and clear comparison between two things .Similes may be confused with metaphors, which do the same kind of thing. Similes use comparisons, with the words 'like' or 'as'. Metaphors use indirect comparisons, without the words 'like' or 'as'.\n",
      "\n",
      "Similes:\n",
      "Like a hungry wolf, he ate the food.\n",
      "\n",
      "A conceptual metaphor or cognitive metaphor is a metaphor which refers to one domain (group of ideas) in terms of another. For example, treating quantity in terms of direction:\n",
      "Prices are rising.\n",
      "I attacked every weak point in his argument. (Argument as war rather than enquiry or search for truth).\n",
      "Life is a journey.\n",
      "\n",
      "Ad hominem is a Latin word for a type of argument. It is a word often used in rhetoric. Rhetoric is the science of speaking well, and convincing other people of your ideas.\n",
      "\n",
      "Translated to English, ad hominem means against the person. In other words, when someone makes an ad hominem, they are attacking the person they are arguing against, instead of what they are saying.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_rag(\"What is a figure of speech that compares two different things?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9dfcf-fee7-472e-bfd5-718c3d283658",
   "metadata": {},
   "source": [
    "### We tried the 3 other questions and this one worked while the other did not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:azureml_py38_PT_and_TF]",
   "language": "python",
   "name": "conda-env-azureml_py38_PT_and_TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
